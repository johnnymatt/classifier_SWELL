{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c1d9a5-3d0d-4377-8fd6-6f143b3483ec",
   "metadata": {},
   "source": [
    "Created on Friday Feb  3 10:04:17 2023\n",
    "@author: Ioannis Matthaiou, Research Fellow, University of Southampton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f78b82-e41e-4fac-8597-b419874a3a94",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d90662-ca64-42ce-bd8c-208e107c1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#% Load Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import xgboost as xgb\n",
    "import sklearn.preprocessing\n",
    "import sklearn.feature_selection\n",
    "import sklearn.ensemble\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import xgboost\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a8aed-9dbf-4632-a2e0-95f57ada889d",
   "metadata": {},
   "source": [
    "Exploratory data analysis\n",
    "1. Load data\n",
    "2. Assign to DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f83a2-1fde-4995-a4ae-5d71c01683a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "parent_dir = './'\n",
    "\n",
    "df_train = pd.read_csv(parent_dir + './Exercise data exploration/data/final/train.csv') # Data frame of training set\n",
    "df_test = pd.read_csv(parent_dir + './Exercise data exploration/data/final/test.csv') # Data frame of testing set\n",
    "\n",
    "Ntrain, ptrain = df_train.shape # get dimension of training set\n",
    "Ntest, ptest = df_test.shape # get dimension of testing set\n",
    "ptrain = ptrain-2\n",
    "ptest = ptest-2\n",
    "print('Number of features = ' + str(ptrain))\n",
    "print('Number of training samples = ' + str(Ntrain))\n",
    "print('Number of testing samples = ' + str(Ntest))\n",
    "if ptrain != ptest:\n",
    "    print('Number of columns differ on testing and training sets by ', np.abs(ptest-ptrain))\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "print('Training set first few rows')\n",
    "print('---------------------------')\n",
    "print(df_train.head(3)) # Load first 3 rows of the training set\n",
    "print('Testing set first few rows')\n",
    "print('---------------------------')\n",
    "print(df_test.head(3)) # Load first 3 rows of the training set\n",
    "# drop datasetId as it is a constant\n",
    "df_train = df_train.drop(columns=[\"datasetId\"])\n",
    "df_test = df_test.drop(columns=[\"datasetId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d39616-0b50-4c9c-94da-ce3f91533236",
   "metadata": {},
   "source": [
    "Datasets summary statistics and fractions of each class:\n",
    "1. Dataset may be slighty imbalanced in terms of classes\n",
    "2. Very different scales of each feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05965928-7892-4125-b382-08fe3ed50b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set summary statistics')\n",
    "print('-------------------------------')\n",
    "print(df_train.describe()) # Descriptive statistics in training set\n",
    "print('Testing set summary statistics')\n",
    "print('-------------------------------')\n",
    "print(df_test.describe()) # Descriptive statistics in training set\n",
    "\n",
    "print('Ratio of test set to train set is ' + str(np.round(len(df_test)/len(df_train),decimals=2)))\n",
    "\n",
    "Ncntstrain = df_train['condition'].value_counts()\n",
    "Ncntstest = df_test['condition'].value_counts()\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "df_train.condition.value_counts(normalize=True).sort_values().plot(kind = 'barh', ax=ax1)\n",
    "ax1.set_title('Fraction of samples per each condition in training set')\n",
    "df_test.condition.value_counts(normalize=True).sort_values().plot(kind = 'barh', ax=ax2)\n",
    "ax2.set_title('Fraction of samples per each condition in testing set')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checks if missing values are present\n",
    "if df_train.isnull().any().any() == True:\n",
    "    print('Missing values in training set')   \n",
    "if df_test.isnull().any().any() == True:\n",
    "    print('Missing values in testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76428b68-c648-4623-b1e2-afc1b728e24c",
   "metadata": {},
   "source": [
    "Visualise data distributions by plotting four different features:\n",
    "1. It is clear that Gaussian distribution assumption is violated in most of the cases\n",
    "2. Amount of bandwidth in kernel density estimates is quite important to avoid oversmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785801c2-db3b-4c26-b057-f02a3a0d774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdebw = 3\n",
    "fig, axes = plt.subplots(nrows=1,ncols=4,sharey=False,figsize = (12, 3))         \n",
    "sns.kdeplot(data=df_train,x=df_train['RMSSD'],\n",
    "                        fill=True, common_norm=False, ax=axes[0],alpha=.25, \n",
    "                        bw_method='scott',bw_adjust=kdebw,legend=True) \n",
    "sns.kdeplot(data=df_train,x=df_train['MEDIAN_RR'],\n",
    "                        fill=True, common_norm=False, ax=axes[1],alpha=.25, \n",
    "                        bw_method='scott',bw_adjust=kdebw,legend=True) \n",
    "sns.kdeplot(data=df_train,x=df_train['LF_HF'],\n",
    "                        fill=True, common_norm=False, ax=axes[2],alpha=.25, \n",
    "                        bw_method='scott',bw_adjust=kdebw,legend=True)   \n",
    "sns.kdeplot(data=df_train,x=df_train['VLF'],\n",
    "                        fill=True, common_norm=False, ax=axes[3],alpha=.25, \n",
    "                        bw_method='scott',bw_adjust=kdebw,legend=True)                  \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626807fd-9539-44ad-9f02-14e8e82b4b76",
   "metadata": {},
   "source": [
    "Scale data, so that each feature is on the same scale:\n",
    "1. Scale features by z-score standardisation if their close approximation to Gaussian dist.\n",
    "2. Min Max is a more general scaling technique that is also not influenced by outliers\n",
    "Encode labels into nominal variables, so that it is compatible with most classification algorithm implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce66b7-da68-4ed4-813a-796759d2fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "# transformed the features\n",
    "X_train = scaler.fit_transform(df_train[df_train.columns[:ptrain]].to_numpy())\n",
    "df_train_sc = pd.DataFrame(X_train, columns=df_train.columns[:ptrain])\n",
    "print(\"Normalised Features:\\n\", df_train_sc[:3])\n",
    "df_train_sc = pd.concat([df_train_sc, df_train['condition']], 1)\n",
    "labenc = sklearn.preprocessing.LabelEncoder()\n",
    "y_train = labenc.fit_transform(df_train['condition'])\n",
    "\n",
    "X_test = scaler.transform(df_test[df_test.columns[:ptest]].to_numpy())\n",
    "y_test = labenc.fit_transform(df_test['condition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0af8a-1df2-4c9e-a6eb-c8d6a4ebb21c",
   "metadata": {},
   "source": [
    "Run boxplotsub function to be used later on (twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb483109-6b18-4739-b6dc-89e4957e0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplotsub(dframe):      \n",
    "    k2=17\n",
    "    nrows = 2\n",
    "    ncols = 1\n",
    "    fig, axes = plt.subplots(nrows=nrows,ncols=ncols,sharey=False,figsize = (10, 25)) \n",
    "    # Feature batch kk:\n",
    "    for i in range(nrows):          \n",
    "        print(range((i * k2),(i * k2) + k2,1))\n",
    "        df_tmp = dframe[dframe.columns[range((i * k2),(i * k2) + k2,1)]] \n",
    "        sns.boxplot(data=df_tmp, ax=axes[i],\n",
    "                    showmeans=True, meanprops={\"marker\":\"o\",\n",
    "                                               \"markerfacecolor\":\"white\", \n",
    "                                               \"markeredgecolor\":\"black\",\n",
    "                                               \"markersize\":\"10\"})   \n",
    "        axes[i].tick_params(labelrotation=90)        \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    fig.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485a608-e391-440c-8b82-b47b17d83cd8",
   "metadata": {},
   "source": [
    "Visualise features better by looking at their scaled versions (compare with the original box plots):\n",
    "1. Many of the features have extreme values and thus are highly skewed\n",
    "2. Can make attempt to remove them if and only if classification accuracy seems to be suffering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbb25e-8586-45ac-af91-a1860bd86197",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplotsub(df_train)\n",
    "boxplotsub(df_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb44d6-1182-4fa9-a546-6d7df8217a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_perctl = .1\n",
    "hi_perctl = .9\n",
    "remvout = 0 # 1: remove/replace outliers, 0: not remove/replace outliers\n",
    "def uvoutlremv(dfc,loper,hiper,chk):\n",
    "    q1 = dfc.quantile(loper)\n",
    "    q3 = dfc.quantile(hiper)\n",
    "    iqr = q3-q1\n",
    "    loval = q1-iqr\n",
    "    hival = q3+iqr\n",
    "    print('Skewness before outlier replacement: ', np.round(dfc.skew(),1))\n",
    "    print('Maximum value before outlier replacement: ', np.round(dfc.max(),1))  \n",
    "    print('Minimum value before outlier replacement: ', np.round(dfc.min(),1))  \n",
    "    if chk==1:\n",
    "        print('Number of outliers removed: ', len(dfc.loc[(dfc<loval) | (dfc>hival)]))\n",
    "        # dfc.loc[(dfc<loval) | (dfc>hival)] = dfc.median()      \n",
    "        dfc.loc[(dfc<loval) | (dfc>hival)] = np.nan\n",
    "        print('Skewness after outlier replacement: ', np.round(dfc.skew(),1)) \n",
    "        print('Maximum value after outlier replacement: ', np.round(dfc.max(),1))  \n",
    "        print('Minimum value after outlier replacement: ', np.round(dfc.min(),1))\n",
    "    else:\n",
    "        print('Number of outliers found: ', len(dfc.loc[(dfc<loval) | (dfc>hival)]))         \n",
    "    return dfc\n",
    "\n",
    "df_train['VLF'] = uvoutlremv(dfc=df_train['VLF'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['SD2'] = uvoutlremv(dfc=df_train['SD2'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['MEAN_RR'] = uvoutlremv(dfc=df_train['MEAN_RR'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['MEDIAN_RR'] = uvoutlremv(dfc=df_train['MEDIAN_RR'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['TP'] = uvoutlremv(dfc=df_train['TP'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['KURT'] = uvoutlremv(dfc=df_train['KURT'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train['LF_HF'] = uvoutlremv(dfc=df_train['LF_HF'],loper=lo_perctl,hiper=hi_perctl,chk=remvout)\n",
    "df_train_sc = df_train_sc.dropna(axis = 0)\n",
    "print('-------OUTLIERS REMOVED : ', len(df_train)-len(df_train_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96f045-e53a-41aa-aa14-a28e3bbe2949",
   "metadata": {},
   "source": [
    "Rescale and replot the distribution of classes if remvout = 1 and reassign to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24430351-4b59-4ff5-9b71-931c920d4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remvout==1:\n",
    "    boxplotsub(df_train_sc)\n",
    "    # transformed the feature\n",
    "    X_train = scaler.fit_transform(df_train[df_train.columns[:ptrain]].to_numpy())\n",
    "    df_train_sc = pd.DataFrame(X_train, columns=df_train.columns[:ptrain])\n",
    "    print(\"Normalised Features:\\n\", df_train_sc[:3])\n",
    "    df_train_sc = pd.concat([df_train_sc, df_train['condition']], 1)\n",
    "    labenc = sklearn.preprocessing.LabelEncoder()\n",
    "    y_train = labenc.fit_transform(df_train['condition'])\n",
    "    Ncntstrain = df_train_sc['condition'].value_counts()\n",
    "    df_train_sc.condition.value_counts(normalize=True).sort_values().plot(kind = 'barh')\n",
    "    ax1.set_title('Fraction of samples per each condition in training set')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    y_train = labenc.fit_transform(df_train_sc['condition'])\n",
    "    X_train = df_train_sc[df_train_sc.columns[:ptrain]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2178764-2be6-4036-9721-29953d2c25d0",
   "metadata": {},
   "source": [
    "Density plots to summarise data in terms of their distributional properties:\n",
    "1. Plot KDE of each feature conditioned on class labels ('condition')\n",
    "2. Check how each feature is distributed when conditioned on the class label and whether there is any change\n",
    "3. It is clear that features are sensitive to the class - either being shifted in x or its peak value changes\n",
    "4. Some of the features conform to a power law: may need to log-transform them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b6d8e-aa89-4fe0-95d7-470fc311de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "Nkdeplots = int(np.ceil(ptrain/(nrows*ncols)))\n",
    "\n",
    "for kk in range(Nkdeplots):\n",
    "    fig, axes = plt.subplots(nrows=nrows,ncols=ncols,sharey=False,figsize = (30, 25))\n",
    "    # Feature batch kk:\n",
    "    for i in range(nrows):\n",
    "        if kk == 3 and i == 2:\n",
    "            ncols = ncols-2\n",
    "        for j in range(ncols):            \n",
    "            sns.kdeplot(data=df_train_sc,x=df_train_sc.columns[k],hue='condition',\n",
    "                        fill=True, common_norm=False, ax=axes[i,j],alpha=.25, \n",
    "                        bw_method='scott',bw_adjust=kdebw,legend=True)   \n",
    "            k += 1          \n",
    "    fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a036b0-67a3-41df-9723-ea6e0efa42f3",
   "metadata": {},
   "source": [
    "View feature-feature linear correlation\n",
    "1. Add a threshold to check only those that are significant\n",
    "2. If needed we can run correlation function to remove the most correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071ec60-0d9f-4e07-8656-6bfcf5564ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {‘pearson’, ‘kendall’, ‘spearman’}\n",
    "# pearson : standard correlation coefficient\n",
    "# kendall : Kendall Tau correlation coefficient\n",
    "# spearman : Spearman rank correlation\n",
    "cut_off = 0.9\n",
    "corrmat = df_train_sc[df_train_sc.columns[:ptrain]].corr(method='pearson')\n",
    "sns.set_theme(style=\"white\")\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corrmat, dtype=bool))\n",
    "mask |= np.abs(corrmat) < cut_off\n",
    "corrmat = corrmat[~mask]\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corrmat, mask=mask, cmap='BrBG', vmin=-1, vmax=1,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .25})\n",
    "plt.show()\n",
    "\n",
    "# Feature selection\n",
    "# Script that can be used to remove a feature that is correlated with any other feature\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr(method='pearson')\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "# df_train_sc.drop(corr_features,axis=1) # do the same for the testing set\n",
    "\n",
    "corr_features = correlation(df_train_sc, cut_off)\n",
    "print('Highly correlated set of features is: ', corr_features,' according to cut_off of ',cut_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322e9f4-8e81-43a3-8c90-d9871601ceff",
   "metadata": {},
   "source": [
    "Feature selection by means of:\n",
    "1. Anova F-test (Linear and Gaussian distributed data)\n",
    "2. Mutual information (Does not assume linearity nor Gaussianity)\n",
    "3. Random Forest classifier via Mean Impurity Decrease (Tied to the method)\n",
    "4. XGBoost classifier via Built-in Feature Importance (Tied to the method)\n",
    "\n",
    "Generally, HR is found to be a good feature that discriminates between the three classes. While ANOVA F-test and Mean Impurity Decrese from the Random Forest yield similarities, e.g. MEAN_RR is found by both as being important or statistically significant (in the F-test scenario).\n",
    "Although in features are linearly correlated (e.g. MEAN_RR - MEDIAN_RR), as seen above, a Random Forest or XGBoost algorithm assigns feature importance, so that it also automatically deals with the problem of feature selection. That is of course not to say that these algorithms can deal with a large number of correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1e4bd-685f-44fc-90ad-b2829406b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information\n",
    "mi_score = sklearn.feature_selection.mutual_info_classif(X_train,y_train)\n",
    "mi_score /= mi_score.max()\n",
    "# ANOVA \n",
    "Fscore,Fpval = sklearn.feature_selection.f_classif(X_train,y_train)\n",
    "Fscore /= Fscore.max()\n",
    "#Random forest feature importance\n",
    "Ntrees = 25\n",
    "modelRF = sklearn.ensemble.RandomForestClassifier(n_estimators = Ntrees, n_jobs=-1)\n",
    "modelRF.fit(X_train, y_train)\n",
    "# get importance\n",
    "RFscore = modelRF.feature_importances_\n",
    "RFscore /= RFscore.max()\n",
    "# XGBoost feature importance\n",
    "modelXGB = xgboost.XGBClassifier(n_estimators = Ntrees, n_jobs=-1)\n",
    "modelXGB.fit(X_train, y_train)\n",
    "# get importance\n",
    "XGBscore = modelXGB.feature_importances_\n",
    "XGBscore /= XGBscore.max()\n",
    "\n",
    "# Plot scores of feature selection\n",
    "df_FSscores=pd.DataFrame({'Mutual Inf':mi_score,\n",
    "                          'F Score':Fscore,\n",
    "                          'RF Score':RFscore,\n",
    "                          'XGB Score':XGBscore,\n",
    "                          'Feature':df_train_sc.columns[:ptrain]})\n",
    "df_FSscores.set_index('Feature', inplace = True)\n",
    "df_FSscores.sort_values('XGB Score', inplace = True, ascending = False)\n",
    "fig, ax = plt.subplots(ncols=1,nrows=4,sharey=True,sharex=True,figsize = (10, 15))\n",
    "sns.lineplot(x='Feature', y='Mutual Inf', data=df_FSscores, ax=ax[0])\n",
    "sns.lineplot(x='Feature', y='F Score', data=df_FSscores, ax=ax[1])\n",
    "sns.lineplot(x='Feature', y='RF Score', data=df_FSscores, ax=ax[2])\n",
    "sns.lineplot(x='Feature', y='XGB Score', data=df_FSscores, ax=ax[3])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.grid(visible=True,which='major',axis='y')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9f31e-77f5-4a81-b3cb-4a9c9acf6039",
   "metadata": {
    "tags": []
   },
   "source": [
    "Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95966a8-01dc-4691-9c7b-d921d9ff8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF.fit(X_train, y_train)\n",
    "yhat = modelRF.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(\"Accuracy in test set = \", (accuracy * 100.0), \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57442eb4-62a2-48a0-9034-1356e3fbc63b",
   "metadata": {},
   "source": [
    "Compare with the simplest classifier - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18ed1b-d3f9-4694-98a7-3f820b0b05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "yhat = cross_val_predict(knn, X_train, y_train, cv=kfold)\n",
    "print(\"Accuracy in training set = \", (accuracy_score(y_train, yhat) * 100.0), \" %\")\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "yhat = knn.predict(X_test)\n",
    "print(\"Accuracy in test set = \", (accuracy_score(y_test, yhat) * 100.0), \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc171c-4652-423f-a0bc-6f475b36ce34",
   "metadata": {},
   "source": [
    "Test decrease in classification accuracy by removing important features computed by Random Forest :\n",
    "As can be seen we can still have close to 100% accuracy on the test set even with 7 or 8 features, i.e. the top ones selected by the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8377a63-defd-4f44-8c93-f96f3c82e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featsimportances = np.sort(modelRF.feature_importances_)\n",
    "for rfimp in featsimportances:\n",
    "\t# select features from model\n",
    "\tFS_sel = SelectFromModel(modelRF, threshold=rfimp, prefit=True)\n",
    "\tX_train_red = FS_sel.transform(X_train)\n",
    "\t# classification model training\n",
    "\tMODEL_C = xgboost.XGBClassifier(n_estimators = Ntrees, n_jobs=-1)\n",
    "\tMODEL_C.fit(X_train_red, y_train)\n",
    "\t# classification model evaluation\n",
    "\tX_test_red = FS_sel.transform(X_test)\n",
    "\tyhat = MODEL_C.predict(X_test_red)\n",
    "\tprint('Trained with =',X_train_red.shape[1], 'number of features')\n",
    "\tprint('Threshold=',rfimp, 'Accuracy=', accuracy_score(y_test, yhat)*100.0,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f2603-38a7-49a3-a029-0e011151751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
